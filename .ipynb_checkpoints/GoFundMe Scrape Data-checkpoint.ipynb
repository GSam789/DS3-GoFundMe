{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = pd.read_csv('GFM_url_list.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Url\", \"Category\",\"Position\", \"Title\", \"Location\",\"Amount_Raised\", \"Goal\", \"Number_of_Donators\", \"Length_of_Fundraising\", \"FB_Shares\", \"GFM_hearts\", \"Text\"]\n",
    "mydf = mydf.reindex(columns = headers)\n",
    "\n",
    "full_df = pd.DataFrame(columns = headers)\n",
    "#need to scrape a single url now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(row_index):\n",
    "    single_row = mydf.iloc[row_index]\n",
    "    url = single_row[\"Url\"]\n",
    "    category = single_row[\"Category\"]\n",
    "    position = single_row[\"Position\"]\n",
    "    \n",
    "    page = requests.get(url)\n",
    "         \n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    #contains amount raised - goal amount - # of donators - length of fundraising\n",
    "    try:\n",
    "        container = soup.find_all(\"div\",{\"class\":\"layer-white hide-for-large mb10\"})\n",
    "        info_string = container[0].text\n",
    "        info_string = info_string.splitlines()\n",
    "        info_string = list(filter(None, info_string))\n",
    "        \n",
    "        amount_raised = int(info_string[0][1:].replace(',',''))\n",
    "        \n",
    "        goal = re.findall('\\$(.*?) goal', info_string[1])[0]\n",
    "        \n",
    "        NumDonators = re.findall('by (.*?) people', info_string[2])[0]\n",
    "        \n",
    "        timeFundraised = re.findall(\"in (.*)$\", info_string[2])[0]\n",
    "    except:\n",
    "        amount_raised = np.nan\n",
    "        goal = np.nan\n",
    "        NumDonators = np.nan\n",
    "        timeFundraised = np.nan\n",
    "        \n",
    "    \n",
    "    title_container = soup.find_all(\"h1\",{\"class\":\"a-campaign-title\"})#<h1 class=\"campaign-title\">Help Rick Muchow Beat Cancer</h1>\n",
    "    \n",
    "    try:\n",
    "        title = title_container[0].text\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    text_container =  soup.find('meta', attrs={'name': 'description'})\n",
    "    \n",
    "    try:\n",
    "        all_text = text_container['content']\n",
    "    except:\n",
    "        all_text = np.nan\n",
    "    \n",
    "    try:\n",
    "        FB_shares_container = soup.find_all(\"strong\", {\"class\":\"js-share-count-text\"})\n",
    "        FB_shares = FB_shares_container[0].text.splitlines()\n",
    "        FB_shares = FB_shares[1].replace(\" \", \"\").replace(\"\\xa0\", \"\")\n",
    "    except:\n",
    "        FB_shares = np.nan\n",
    "        \n",
    "    try:\n",
    "        hearts_container = soup.find_all(\"div\", {\"class\":\"campaign-sp campaign-sp--heart fave-num\"})\n",
    "        hearts = hearts_container[0].text\n",
    "    except:\n",
    "        hearts = np.nan\n",
    "    \n",
    "    try:\n",
    "        location_container = soup.find_all(\"div\", {\"class\":\"pills-contain\"})\n",
    "        location = location_container[0].text.splitlines()[-1]\n",
    "        location = location.replace('\\xa0', '').strip()\n",
    "    except:\n",
    "        location = np.nan\n",
    "        \n",
    "    temp_row = np.array([[url, category, position, title, location, amount_raised, goal, NumDonators, timeFundraised, FB_shares, hearts, all_text]])\n",
    "    temp_df = pd.DataFrame(temp_row, columns = headers)\n",
    "    \n",
    "    return(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_urls(file = 'GFM_data.csv', start = 0, end = len(mydf)):\n",
    "    for i in range(start, end):\n",
    "        try:\n",
    "            temp_df = scrape_url(i)\n",
    "            temp_df.to_csv(file, mode = 'a',sep = '\\t', header = False)\n",
    "            print(\"Scraping url %s\" %(i+1))\n",
    "        except:\n",
    "            sleep(5)\n",
    "            temp_df = scrape_url(i)\n",
    "            temp_df.to_csv(file, mode = 'a',sep = '\\t', header = False)\n",
    "            print(\"Scraping url %s\" %(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping url 1\n",
      "Scraping url 2\n",
      "Scraping url 3\n",
      "Scraping url 4\n",
      "Scraping url 5\n",
      "Scraping url 6\n",
      "Scraping url 7\n",
      "Scraping url 8\n",
      "Scraping url 9\n"
     ]
    }
   ],
   "source": [
    "scrape_all_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
