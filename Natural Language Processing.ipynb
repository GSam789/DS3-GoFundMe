{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict # Dictionaries with default values\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Url</th>\n",
       "      <th>Category</th>\n",
       "      <th>Position</th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Amount_Raised</th>\n",
       "      <th>Goal</th>\n",
       "      <th>Number_of_Donators</th>\n",
       "      <th>Length_of_Fundraising</th>\n",
       "      <th>FB_Shares</th>\n",
       "      <th>GFM_hearts</th>\n",
       "      <th>Text</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/3ctqm-medical-bills-f...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>92 Yr old Man Brutally Attacked.</td>\n",
       "      <td>LOS ANGELES, CA</td>\n",
       "      <td>327345.0</td>\n",
       "      <td>15,000</td>\n",
       "      <td>12,167</td>\n",
       "      <td>1 month</td>\n",
       "      <td>26k</td>\n",
       "      <td>12k</td>\n",
       "      <td>Rodolfo Rodriguez needs your help today! 92 Yr...</td>\n",
       "      <td>34.052234</td>\n",
       "      <td>-118.243685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.gofundme.com/olivia-stoy-bone-marr...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Olivia Stoy:Transplant &amp; Liv it up!</td>\n",
       "      <td>ASHLEY, IN</td>\n",
       "      <td>316261.0</td>\n",
       "      <td>1.0M</td>\n",
       "      <td>5,598</td>\n",
       "      <td>3 months</td>\n",
       "      <td>12k</td>\n",
       "      <td>5.7k</td>\n",
       "      <td>Thomas Stoy needs your help today! Olivia Stoy...</td>\n",
       "      <td>41.527272</td>\n",
       "      <td>-85.065523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.gofundme.com/autologous-Tcell-Tran...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOLOGOUS T CELL TRANSPLANT</td>\n",
       "      <td>STATEN ISLAND, NY</td>\n",
       "      <td>241125.0</td>\n",
       "      <td>250,000</td>\n",
       "      <td>841</td>\n",
       "      <td>2 months</td>\n",
       "      <td>1.8k</td>\n",
       "      <td>836</td>\n",
       "      <td>Philip Defonte needs your help today! AUTOLOGO...</td>\n",
       "      <td>40.579532</td>\n",
       "      <td>-74.150201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.gofundme.com/a-chance-of-rebirth</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>A chance of rebirth</td>\n",
       "      <td>DUBLIN, CA</td>\n",
       "      <td>237424.0</td>\n",
       "      <td>225,000</td>\n",
       "      <td>4,708</td>\n",
       "      <td>1 month</td>\n",
       "      <td>9.7k</td>\n",
       "      <td>4.7k</td>\n",
       "      <td>Sriram Kanniah needs your help today! A chance...</td>\n",
       "      <td>37.702152</td>\n",
       "      <td>-121.935792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.gofundme.com/teamclaire</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>Claire Wineland Needs Our Help</td>\n",
       "      <td>GARDEN GROVE, CA</td>\n",
       "      <td>236590.0</td>\n",
       "      <td>225,000</td>\n",
       "      <td>8,393</td>\n",
       "      <td>2 months</td>\n",
       "      <td>6.4k</td>\n",
       "      <td>8.9k</td>\n",
       "      <td>Melissa Yeager needs your help today! Claire W...</td>\n",
       "      <td>33.774269</td>\n",
       "      <td>-117.937995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                Url Category  \\\n",
       "0           0  https://www.gofundme.com/3ctqm-medical-bills-f...  Medical   \n",
       "1           1  https://www.gofundme.com/olivia-stoy-bone-marr...  Medical   \n",
       "2           2  https://www.gofundme.com/autologous-Tcell-Tran...  Medical   \n",
       "3           3       https://www.gofundme.com/a-chance-of-rebirth  Medical   \n",
       "4           4                https://www.gofundme.com/teamclaire  Medical   \n",
       "\n",
       "  Position                                Title           Location  \\\n",
       "0        0     92 Yr old Man Brutally Attacked.    LOS ANGELES, CA   \n",
       "1        0  Olivia Stoy:Transplant & Liv it up!         ASHLEY, IN   \n",
       "2        1         AUTOLOGOUS T CELL TRANSPLANT  STATEN ISLAND, NY   \n",
       "3        1                  A chance of rebirth         DUBLIN, CA   \n",
       "4        1       Claire Wineland Needs Our Help   GARDEN GROVE, CA   \n",
       "\n",
       "   Amount_Raised     Goal Number_of_Donators Length_of_Fundraising FB_Shares  \\\n",
       "0       327345.0   15,000             12,167               1 month       26k   \n",
       "1       316261.0     1.0M              5,598              3 months       12k   \n",
       "2       241125.0  250,000                841              2 months      1.8k   \n",
       "3       237424.0  225,000              4,708               1 month      9.7k   \n",
       "4       236590.0  225,000              8,393              2 months      6.4k   \n",
       "\n",
       "  GFM_hearts                                               Text   Latitude  \\\n",
       "0        12k  Rodolfo Rodriguez needs your help today! 92 Yr...  34.052234   \n",
       "1       5.7k  Thomas Stoy needs your help today! Olivia Stoy...  41.527272   \n",
       "2        836  Philip Defonte needs your help today! AUTOLOGO...  40.579532   \n",
       "3       4.7k  Sriram Kanniah needs your help today! A chance...  37.702152   \n",
       "4       8.9k  Melissa Yeager needs your help today! Claire W...  33.774269   \n",
       "\n",
       "    Longitude  \n",
       "0 -118.243685  \n",
       "1  -85.065523  \n",
       "2  -74.150201  \n",
       "3 -121.935792  \n",
       "4 -117.937995  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing data sets and dropping nan's\n",
    "df = pd.read_csv('GFM_data.csv',sep = '\\t')\n",
    "df = df.loc[df['Text'].dropna().index]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "\n",
    "def extract_entities(text):\n",
    "    names = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                names.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "    new_text = text\n",
    "    for name in names:\n",
    "        if name in text:\n",
    "            new_text = new_text.replace(name, 'NLP')\n",
    "    return new_text\n",
    "\n",
    "def clean_text(x):\n",
    "    ## removing names\n",
    "    x = extract_entities(x)\n",
    "    ## normalizing text by stripping white space and lower casing\n",
    "    x =  x.lower().strip()\n",
    "    ## removing urls\n",
    "    x = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', x)\n",
    "    ## removing phone numbers\n",
    "    x = re.sub('\\([0-9]{3}\\)\\s*[0-9]{3}-[0-9]{4}','',x)\n",
    "    ## strip all non alphanumeric things\n",
    "    x = re.sub('\\n',' ',x)\n",
    "    x = re.sub(\"[^a-zA-Z0-9 #]\",'',x)\n",
    "    x = re.sub(\"\\s+\",' ',x)\n",
    "    text = x.replace('\\n', ' ').lower()# lowercase text\n",
    "    text = REPLACE_IP_ADDRESS.sub('', text) # remove ip address\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([w for w in text.split() if not w in STOPWORDS])# delete stopwords from text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melissa Yeager needs your help today! Claire Wineland Needs Our Help - First of all, WOW!! We have been blown away by the amazing outpouring of love and support you have shown to Claire..deeply humbled in fact.Â  Friends have encouraged us to raise the goal to meet the demand but I want everyone to know that our transplant finances have been covered in full.Â  Anythin...\n",
      "\n",
      "nlp nlp needs help today nlp nlp needs nlp first nlp blown away amazing outpouring love support shown nlpdeeply humbled fact friends encouraged us raise goal meet demand want everyone know transplant finances covered full nlp\n"
     ]
    }
   ],
   "source": [
    "text = df['Text'][4]\n",
    "new_text = clean_text(text)\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "print(text)\n",
    "print()\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {i: idx for idx,i in enumerate(df['Category'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['Text']\n",
    "y = [categories[i] for i in df['Category']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW (Bag of Words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "for comments in X_train:\n",
    "    for word in comments.split():\n",
    "        if word not in words_counts:\n",
    "            words_counts[word] = 0\n",
    "        words_counts[word] += 1\n",
    "##most pop words        \n",
    "DICT_SIZE = 10000\n",
    "POPULAR_WORDS = sorted(words_counts, key=words_counts.get, reverse=True)[:DICT_SIZE]\n",
    "## same dics but flopped\n",
    "WORDS_TO_INDEX = {key: rank for rank, key in enumerate(POPULAR_WORDS, 0)}\n",
    "INDEX_TO_WORDS = {index:word for word, index in WORDS_TO_INDEX.items()}\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (927, 10000) \n",
      "X_val shape  (310, 10000)\n"
     ]
    }
   ],
   "source": [
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    for word in text.split(' '):\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]] +=1\n",
    "    return result_vector\n",
    "\n",
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape, '\\nX_val shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t8.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 116)\t1.0\n",
      "  (0, 214)\t1.0\n",
      "  (0, 347)\t1.0\n",
      "  (0, 348)\t3.0\n",
      "  (0, 523)\t2.0\n",
      "  (0, 686)\t2.0\n",
      "  (0, 805)\t1.0\n",
      "  (0, 980)\t2.0\n",
      "  (0, 1234)\t3.0\n",
      "  (0, 1235)\t1.0\n",
      "  (0, 1236)\t2.0\n",
      "  (0, 1237)\t1.0\n",
      "  (0, 1687)\t1.0\n",
      "  (0, 1688)\t1.0\n",
      "  (0, 1689)\t1.0\n",
      "  (0, 1690)\t1.0\n",
      "  (0, 2549)\t1.0\n",
      "  (0, 2550)\t1.0\n",
      "  (0, 2551)\t1.0\n",
      "  (0, 2552)\t1.0\n",
      "  (0, 2553)\t1.0\n",
      "  (0, 2554)\t1.0\n",
      "  (0, 2555)\t1.0\n",
      "  (0, 2556)\t1.0\n",
      "  (0, 2557)\t1.0\n",
      "  (0, 2558)\t1.0\n",
      "  (0, 2559)\t1.0\n",
      "  (0, 2560)\t1.0\n",
      "  (0, 2561)\t1.0\n",
      "  (0, 2562)\t1.0\n",
      "  (0, 2563)\t1.0\n",
      "  (0, 2564)\t1.0\n",
      "  (0, 2565)\t1.0\n",
      "  (0, 2566)\t1.0\n",
      "  (0, 2567)\t1.0\n",
      "  (0, 2568)\t1.0\n",
      "  (0, 2569)\t1.0\n",
      "  (0, 2570)\t1.0\n",
      "  (0, 2571)\t1.0\n",
      "  (0, 2572)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train_mybag[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp',\n",
       " 'help',\n",
       " 'needs',\n",
       " 'today',\n",
       " 'nlps',\n",
       " 'family',\n",
       " 'years',\n",
       " '2018',\n",
       " 'us',\n",
       " 'support']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POPULAR_WORDS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating tfidf vector\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5)\n",
    "## transforming it\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 663)\t0.18353528942150774\n",
      "  (0, 553)\t0.16712875883324108\n",
      "  (0, 1185)\t0.20182236858230343\n",
      "  (0, 928)\t0.6729898429180977\n",
      "  (0, 563)\t0.4372117722921825\n",
      "  (0, 326)\t0.4486598952787318\n",
      "  (0, 783)\t0.0419088341372966\n",
      "  (0, 1126)\t0.052349394245470186\n",
      "  (0, 554)\t0.21860588614609125\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "def train_classifier(X_train, y_train, C, regularisation):\n",
    "    model = OneVsRestClassifier(LogisticRegression(penalty=regularisation, C=C, max_iter=10000)).fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gauri\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train, C = 4, regularisation = 'l2')\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train, C = 4, regularisation = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
    "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_scores_mybag = classifier_mybag.decision_function(X_test_mybag)\n",
    "y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.8984405 ,  -4.56843965,  -7.20291397, ...,  -7.79626134,\n",
       "         -5.21450572,  -4.88280427],\n",
       "       [ -5.88841381,  -7.92511479,  -3.33961138, ...,  -3.35376249,\n",
       "         -4.3727636 ,  -2.72938289],\n",
       "       [-10.46610227,  -0.80775191,  -6.07997797, ...,  -5.10231465,\n",
       "         -6.84914677,  -5.86018039],\n",
       "       ...,\n",
       "       [ -4.59829906,  -5.85600628,  -4.57474017, ...,  -6.36047193,\n",
       "         -2.94255695,  -4.14086362],\n",
       "       [ -6.46634004,  -7.75027993,  -4.40796338, ...,  -3.39192914,\n",
       "         -4.30187695,  -5.77795586],\n",
       "       [ -6.10529502,  -6.06100736,  -7.17042775, ...,  -2.57392758,\n",
       "         -6.64355046,  -5.60570594]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted_scores_mybag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "\n",
      "Accuracy:  102\n",
      "F1-score weighted:  0.331068799235176\n",
      "\n",
      "Tfidf\n",
      "\n",
      "Accuracy:  96\n",
      "F1-score weighted:  0.3110173676505555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gauri\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train, C = 50, regularisation = 'l2')\n",
    "\n",
    "y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
    "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "def print_evaluation_scores(y_test, predicted):\n",
    "    \n",
    "    print('Accuracy: ', accuracy_score(y_test, predicted, normalize=False))\n",
    "    print('F1-score weighted: ', f1_score(y_test, predicted, average='weighted'))\n",
    "    \n",
    "print('Bag-of-words\\n')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_mybag)\n",
    "print('\\nTfidf\\n')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
