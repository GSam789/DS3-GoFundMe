{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict # Dictionaries with default values\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Url</th>\n",
       "      <th>Category</th>\n",
       "      <th>Position</th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Amount_Raised</th>\n",
       "      <th>Goal</th>\n",
       "      <th>Number_of_Donations</th>\n",
       "      <th>Length_of_Fundraising</th>\n",
       "      <th>FB_Shares</th>\n",
       "      <th>Number_of_Donors</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/justiceforjacobblake</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Justice for Jacob Blake</td>\n",
       "      <td>Kenosha, WI</td>\n",
       "      <td>2297930.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>73K</td>\n",
       "      <td>93 days 12:02:38.405126000</td>\n",
       "      <td>118K</td>\n",
       "      <td>72.5K</td>\n",
       "      <td>73.4K</td>\n",
       "      <td>On August 23rd my son was shot multiple times ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/official-navajo-nat...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Official Navajo Nation COVID-19 Relief Fund</td>\n",
       "      <td>Window Rock, AZ</td>\n",
       "      <td>1862040.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>22.5K</td>\n",
       "      <td>205 days 12:02:39.366241000</td>\n",
       "      <td>71.7K</td>\n",
       "      <td>21.9K</td>\n",
       "      <td>22K</td>\n",
       "      <td>\\r\\nThe Navajo Nation COVID-19 Fund has been e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/help-a-front-line-n...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Help a front line nurse and baby get proper care</td>\n",
       "      <td>Randolph, NJ</td>\n",
       "      <td>954793.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>19K</td>\n",
       "      <td>215 days 12:02:40.340314000</td>\n",
       "      <td>16.4K</td>\n",
       "      <td>18.3K</td>\n",
       "      <td>17.9K</td>\n",
       "      <td>On Sunday, April 12, Sylvia Leroy, a pregnant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/Tommy-Rivers-Rest-Up</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>Rest up, Tommy, we'll see you soon</td>\n",
       "      <td>Scottsdale, AZ</td>\n",
       "      <td>673179.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>11.3K</td>\n",
       "      <td>131 days 12:02:41.464483000</td>\n",
       "      <td>21.3K</td>\n",
       "      <td>10.3K</td>\n",
       "      <td>10.4K</td>\n",
       "      <td>First, thank you for being here. Tommy Rivers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/brandon039s-medical...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>OFFICIAL BRANDON SAENZ MEDICAL FUND</td>\n",
       "      <td>Tyler, TX</td>\n",
       "      <td>570529.0</td>\n",
       "      <td>750000.0</td>\n",
       "      <td>24.7K</td>\n",
       "      <td>175 days 12:02:42.383091000</td>\n",
       "      <td>5.5K</td>\n",
       "      <td>24.3K</td>\n",
       "      <td>24.5K</td>\n",
       "      <td>My name is Melissa Green and I am the mother o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                Url Category  \\\n",
       "0           0    https://www.gofundme.com/f/justiceforjacobblake  Medical   \n",
       "1           0  https://www.gofundme.com/f/official-navajo-nat...  Medical   \n",
       "2           0  https://www.gofundme.com/f/help-a-front-line-n...  Medical   \n",
       "3           0    https://www.gofundme.com/f/Tommy-Rivers-Rest-Up  Medical   \n",
       "4           0  https://www.gofundme.com/f/brandon039s-medical...  Medical   \n",
       "\n",
       "   Position                                             Title  \\\n",
       "0         0                           Justice for Jacob Blake   \n",
       "1         0       Official Navajo Nation COVID-19 Relief Fund   \n",
       "2         0  Help a front line nurse and baby get proper care   \n",
       "3         1                Rest up, Tommy, we'll see you soon   \n",
       "4         1               OFFICIAL BRANDON SAENZ MEDICAL FUND   \n",
       "\n",
       "          Location  Amount_Raised       Goal Number_of_Donations  \\\n",
       "0      Kenosha, WI      2297930.0  3000000.0                 73K   \n",
       "1  Window Rock, AZ      1862040.0  1000000.0               22.5K   \n",
       "2     Randolph, NJ       954793.0  1200000.0                 19K   \n",
       "3   Scottsdale, AZ       673179.0  1000000.0               11.3K   \n",
       "4        Tyler, TX       570529.0   750000.0               24.7K   \n",
       "\n",
       "         Length_of_Fundraising FB_Shares Number_of_Donors Followers  \\\n",
       "0   93 days 12:02:38.405126000      118K            72.5K     73.4K   \n",
       "1  205 days 12:02:39.366241000     71.7K            21.9K       22K   \n",
       "2  215 days 12:02:40.340314000     16.4K            18.3K     17.9K   \n",
       "3  131 days 12:02:41.464483000     21.3K            10.3K     10.4K   \n",
       "4  175 days 12:02:42.383091000      5.5K            24.3K     24.5K   \n",
       "\n",
       "                                                Text  \n",
       "0  On August 23rd my son was shot multiple times ...  \n",
       "1  \\r\\nThe Navajo Nation COVID-19 Fund has been e...  \n",
       "2  On Sunday, April 12, Sylvia Leroy, a pregnant ...  \n",
       "3  First, thank you for being here. Tommy Rivers ...  \n",
       "4  My name is Melissa Green and I am the mother o...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing data sets and dropping nan's\n",
    "df = pd.read_csv('GFM_data.csv',sep = '\\t')\n",
    "df = df.loc[df['Text'].dropna().index]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "\n",
    "def extract_entities(text):\n",
    "    names = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                names.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "    new_text = text\n",
    "    for name in names:\n",
    "        if name in text:\n",
    "            new_text = new_text.replace(name, 'NLP')\n",
    "    return new_text\n",
    "\n",
    "def clean_text(x):\n",
    "    ## removing names\n",
    "    x = extract_entities(x)\n",
    "    ## normalizing text by stripping white space and lower casing\n",
    "    x =  x.lower().strip()\n",
    "    ## removing urls\n",
    "    x = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', x)\n",
    "    ## removing phone numbers\n",
    "    x = re.sub('\\([0-9]{3}\\)\\s*[0-9]{3}-[0-9]{4}','',x)\n",
    "    ## strip all non alphanumeric things\n",
    "    x = re.sub('\\n',' ',x)\n",
    "    x = re.sub(\"[^a-zA-Z0-9 #]\",'',x)\n",
    "    x = re.sub(\"\\s+\",' ',x)\n",
    "    text = x.replace('\\n', ' ').lower()# lowercase text\n",
    "    text = REPLACE_IP_ADDRESS.sub('', text) # remove ip address\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([w for w in text.split() if not w in STOPWORDS])# delete stopwords from text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Melissa Green and I am the mother of Brandon Saenz.  Brandon is my only son, will always be the baby you see on the right, and sadly his life has forever been changed. On Saturday, May 30, 2020, Brandon was walking to get something to eat in downtown Dallas, and encountered protesters as he walked.  Police officers began walking towards the protestors and with no warning or provocation, they opened fire on the crowd and struck Brandon in his left eye.  Brandon had broken no laws.  Brandon had to undergo surgery where they were unable to salvage his eye and had to remove it.  My son’s entire left side of his face was broken including his nose and jaw. On Wednesday, his 26th birthday, he was released from the hospital and was simply just grateful to be alive.  The officer that committed this heinous crime has been protected by law enforcement and therefore we are unable to hold them accountable at this moment.  Brandon has been permanently maimed and is now disabled.  He has a long way to go with medical and life expenses.  We acknowledge that times are tough and appreciate anything that you can donate for my son.  Thanks you for all of the messages of encouragement as well.  TY SINCERELY BRANDON SAENZ AND FAMILY\n",
      "\n",
      "name nlp mother nlpnlp son always baby see right sadly life forever changed saturday may 30 2020 nlp walking get something eat downtown nlp encountered protesters walkedpolice officers began walking towards protestors warning provocation opened fire crowd struck nlp left eyenlp broken lawsnlp undergo surgery unable salvage eye remove itmy sons entire left side face broken including nose jaw wednesday 26th birthday released hospital simply grateful alivethe officer committed heinous crime protected law enforcement therefore unable hold accountable momentnlp permanently maimed disabledhe long way go medical life expenseswe acknowledge times tough appreciate anything donate sonthanks messages encouragement wellty sincerely nlp saenz family\n"
     ]
    }
   ],
   "source": [
    "text = df['Text'][4]\n",
    "new_text = clean_text(text)\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "print(text)\n",
    "print()\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {i: idx for idx,i in enumerate(df['Category'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['Text']\n",
    "y = [categories[i] for i in df['Category']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW (Bag of Words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "for comments in X_train:\n",
    "    for word in comments.split():\n",
    "        if word not in words_counts:\n",
    "            words_counts[word] = 0\n",
    "        words_counts[word] += 1\n",
    "##most pop words        \n",
    "DICT_SIZE = 10000\n",
    "POPULAR_WORDS = sorted(words_counts, key=words_counts.get, reverse=True)[:DICT_SIZE]\n",
    "## same dics but flopped\n",
    "WORDS_TO_INDEX = {key: rank for rank, key in enumerate(POPULAR_WORDS, 0)}\n",
    "INDEX_TO_WORDS = {index:word for word, index in WORDS_TO_INDEX.items()}\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (646, 10000) \n",
      "X_val shape  (216, 10000)\n"
     ]
    }
   ],
   "source": [
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    for word in text.split(' '):\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]] +=1\n",
    "    return result_vector\n",
    "\n",
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape, '\\nX_val shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t27.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t5.0\n",
      "  (0, 4)\t5.0\n",
      "  (0, 5)\t4.0\n",
      "  (0, 6)\t2.0\n",
      "  (0, 7)\t2.0\n",
      "  (0, 8)\t3.0\n",
      "  (0, 9)\t5.0\n",
      "  (0, 10)\t2.0\n",
      "  (0, 11)\t1.0\n",
      "  (0, 13)\t3.0\n",
      "  (0, 14)\t1.0\n",
      "  (0, 17)\t3.0\n",
      "  (0, 18)\t1.0\n",
      "  (0, 19)\t1.0\n",
      "  (0, 20)\t1.0\n",
      "  (0, 21)\t2.0\n",
      "  (0, 22)\t1.0\n",
      "  (0, 25)\t2.0\n",
      "  (0, 29)\t1.0\n",
      "  (0, 33)\t1.0\n",
      "  (0, 36)\t1.0\n",
      "  (0, 41)\t1.0\n",
      "  (0, 43)\t4.0\n",
      "  :\t:\n",
      "  (0, 6877)\t1.0\n",
      "  (0, 6878)\t1.0\n",
      "  (0, 6879)\t1.0\n",
      "  (0, 6880)\t1.0\n",
      "  (0, 6881)\t1.0\n",
      "  (0, 6882)\t1.0\n",
      "  (0, 6883)\t1.0\n",
      "  (0, 6884)\t1.0\n",
      "  (0, 6885)\t1.0\n",
      "  (0, 6886)\t1.0\n",
      "  (0, 6887)\t1.0\n",
      "  (0, 6888)\t1.0\n",
      "  (0, 6889)\t1.0\n",
      "  (0, 6890)\t1.0\n",
      "  (0, 6891)\t1.0\n",
      "  (0, 6892)\t1.0\n",
      "  (0, 6893)\t1.0\n",
      "  (0, 6894)\t1.0\n",
      "  (0, 6895)\t1.0\n",
      "  (0, 6896)\t1.0\n",
      "  (0, 6897)\t1.0\n",
      "  (0, 6898)\t1.0\n",
      "  (0, 6899)\t1.0\n",
      "  (0, 6900)\t1.0\n",
      "  (0, 6901)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train_mybag[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp',\n",
       " 'help',\n",
       " 'nlps',\n",
       " 'family',\n",
       " 'us',\n",
       " 'support',\n",
       " 'community',\n",
       " 'time',\n",
       " 'many',\n",
       " 'years']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POPULAR_WORDS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating tfidf vector\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5)\n",
    "## transforming it\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3450)\t0.05330902462212366\n",
      "  (0, 92)\t0.10549788875949537\n",
      "  (0, 2298)\t0.057360000299316836\n",
      "  (0, 3442)\t0.04413319827621207\n",
      "  (0, 73)\t0.06351011300383887\n",
      "  (0, 3987)\t0.06839173208596769\n",
      "  (0, 1259)\t0.10371794737266439\n",
      "  (0, 2123)\t0.10355856513852879\n",
      "  (0, 1716)\t0.059599019618880145\n",
      "  (0, 510)\t0.1380780868513717\n",
      "  (0, 1189)\t0.03916272599342111\n",
      "  (0, 2897)\t0.2819240170137015\n",
      "  (0, 114)\t0.06839173208596769\n",
      "  (0, 4201)\t0.0404706289932095\n",
      "  (0, 1625)\t0.03488623698750032\n",
      "  (0, 499)\t0.04262628985226823\n",
      "  (0, 2927)\t0.07832545198684222\n",
      "  (0, 4250)\t0.11865510756390832\n",
      "  (0, 3239)\t0.06351011300383887\n",
      "  (0, 1172)\t0.07502177135205362\n",
      "  (0, 3860)\t0.043401758506643406\n",
      "  (0, 1453)\t0.04942297692192443\n",
      "  (0, 3710)\t0.07886079137268459\n",
      "  (0, 4164)\t0.06653582664522843\n",
      "  (0, 712)\t0.047266276840831505\n",
      "  :\t:\n",
      "  (0, 4269)\t0.06004654914499174\n",
      "  (0, 2626)\t0.09042351305974611\n",
      "  (0, 2429)\t0.06839173208596769\n",
      "  (0, 1262)\t0.13307165329045686\n",
      "  (0, 511)\t0.055853091875373004\n",
      "  (0, 2510)\t0.06839173208596769\n",
      "  (0, 2898)\t0.050737687699131576\n",
      "  (0, 4202)\t0.053896436440469715\n",
      "  (0, 93)\t0.061094127320723626\n",
      "  (0, 2044)\t0.06839173208596769\n",
      "  (0, 3580)\t0.06653582664522843\n",
      "  (0, 1397)\t0.06492816822712057\n",
      "  (0, 4234)\t0.06492816822712057\n",
      "  (0, 2547)\t0.06351011300383887\n",
      "  (0, 3971)\t0.06653582664522843\n",
      "  (0, 4229)\t0.04027485957338593\n",
      "  (0, 2084)\t0.057360000299316836\n",
      "  (0, 2681)\t0.057360000299316836\n",
      "  (0, 2156)\t0.062241619381445665\n",
      "  (0, 2662)\t0.06839173208596769\n",
      "  (0, 4019)\t0.04942297692192443\n",
      "  (0, 2193)\t0.04715891191760164\n",
      "  (0, 3719)\t0.062241619381445665\n",
      "  (0, 1433)\t0.04616880188938068\n",
      "  (0, 3865)\t0.05819064370425248\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "def train_classifier(X_train, y_train, C, regularisation):\n",
    "    model = OneVsRestClassifier(LogisticRegression(penalty=regularisation, C=C, max_iter=10000)).fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gauri\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train, C = 4, regularisation = 'l2')\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train, C = 4, regularisation = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
    "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_scores_mybag = classifier_mybag.decision_function(X_test_mybag)\n",
    "y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.8984405 ,  -4.56843965,  -7.20291397, ...,  -7.79626134,\n",
       "         -5.21450572,  -4.88280427],\n",
       "       [ -5.88841381,  -7.92511479,  -3.33961138, ...,  -3.35376249,\n",
       "         -4.3727636 ,  -2.72938289],\n",
       "       [-10.46610227,  -0.80775191,  -6.07997797, ...,  -5.10231465,\n",
       "         -6.84914677,  -5.86018039],\n",
       "       ...,\n",
       "       [ -4.59829906,  -5.85600628,  -4.57474017, ...,  -6.36047193,\n",
       "         -2.94255695,  -4.14086362],\n",
       "       [ -6.46634004,  -7.75027993,  -4.40796338, ...,  -3.39192914,\n",
       "         -4.30187695,  -5.77795586],\n",
       "       [ -6.10529502,  -6.06100736,  -7.17042775, ...,  -2.57392758,\n",
       "         -6.64355046,  -5.60570594]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted_scores_mybag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "\n",
      "Accuracy:  102\n",
      "F1-score weighted:  0.331068799235176\n",
      "\n",
      "Tfidf\n",
      "\n",
      "Accuracy:  96\n",
      "F1-score weighted:  0.3110173676505555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gauri\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train, C = 50, regularisation = 'l2')\n",
    "\n",
    "y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
    "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "def print_evaluation_scores(y_test, predicted):\n",
    "    \n",
    "    print('Accuracy: ', accuracy_score(y_test, predicted, normalize=False))\n",
    "    print('F1-score weighted: ', f1_score(y_test, predicted, average='weighted'))\n",
    "    \n",
    "print('Bag-of-words\\n')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_mybag)\n",
    "print('\\nTfidf\\n')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
